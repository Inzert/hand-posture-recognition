{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training dataset\n",
    "train_set = pd.read_csv('./HG_Dataset/D_train.csv')\n",
    "\n",
    "#Column names for filtering purposes\n",
    "all_columns = ['X0','Y0','Z0','X1','Y1','Z1','X2','Y2','Z2','X3','Y3','Z3',\\\n",
    "           'X4','Y4','Z4','X5','Y5','Z5','X6','Y6','Z6','X7','Y7','Z7',\\\n",
    "           'X8','Y8','Z8','X9','Y9','Z9','X10','Y10','Z10','X11','Y11','Z11']\n",
    "x_columns = ['X0','X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11']\n",
    "y_columns = ['Y0','Y1','Y2','Y3','Y4','Y5','Y6','Y7','Y8','Y9','Y10','Y11']\n",
    "z_columns = ['Z0','Z1','Z2','Z3','Z4','Z5','Z6','Z7','Z8','Z9','Z10','Z11']\n",
    "final_features = ['count','y_mean','z_mean','x_std','y_std','z_std',\\\n",
    "                 'x_max','y_max','z_max','x_min']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing: Delete the unique identifier column\n",
    "train_set.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>User</th>\n",
       "      <th>X0</th>\n",
       "      <th>Y0</th>\n",
       "      <th>Z0</th>\n",
       "      <th>X1</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Z1</th>\n",
       "      <th>X2</th>\n",
       "      <th>Y2</th>\n",
       "      <th>Z2</th>\n",
       "      <th>X3</th>\n",
       "      <th>Y3</th>\n",
       "      <th>Z3</th>\n",
       "      <th>X4</th>\n",
       "      <th>Y4</th>\n",
       "      <th>Z4</th>\n",
       "      <th>X5</th>\n",
       "      <th>Y5</th>\n",
       "      <th>Z5</th>\n",
       "      <th>X6</th>\n",
       "      <th>Y6</th>\n",
       "      <th>Z6</th>\n",
       "      <th>X7</th>\n",
       "      <th>Y7</th>\n",
       "      <th>Z7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y8</th>\n",
       "      <th>Z8</th>\n",
       "      <th>X9</th>\n",
       "      <th>Y9</th>\n",
       "      <th>Z9</th>\n",
       "      <th>X10</th>\n",
       "      <th>Y10</th>\n",
       "      <th>Z10</th>\n",
       "      <th>X11</th>\n",
       "      <th>Y11</th>\n",
       "      <th>Z11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13464.000000</td>\n",
       "      <td>13464.000000</td>\n",
       "      <td>13464.000000</td>\n",
       "      <td>13254.000000</td>\n",
       "      <td>13254.000000</td>\n",
       "      <td>13254.000000</td>\n",
       "      <td>11363.000000</td>\n",
       "      <td>11363.000000</td>\n",
       "      <td>11363.000000</td>\n",
       "      <td>9616.000000</td>\n",
       "      <td>9616.000000</td>\n",
       "      <td>9616.000000</td>\n",
       "      <td>7191.000000</td>\n",
       "      <td>7191.000000</td>\n",
       "      <td>7191.000000</td>\n",
       "      <td>5733.000000</td>\n",
       "      <td>5733.000000</td>\n",
       "      <td>5733.000000</td>\n",
       "      <td>4537.000000</td>\n",
       "      <td>4537.000000</td>\n",
       "      <td>4537.000000</td>\n",
       "      <td>2482.000000</td>\n",
       "      <td>2482.000000</td>\n",
       "      <td>2482.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.025481</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>52.698144</td>\n",
       "      <td>85.770524</td>\n",
       "      <td>-31.826014</td>\n",
       "      <td>52.351641</td>\n",
       "      <td>86.272265</td>\n",
       "      <td>-31.711906</td>\n",
       "      <td>52.112129</td>\n",
       "      <td>83.898329</td>\n",
       "      <td>-32.956184</td>\n",
       "      <td>51.149287</td>\n",
       "      <td>82.018282</td>\n",
       "      <td>-33.071126</td>\n",
       "      <td>50.755154</td>\n",
       "      <td>80.962652</td>\n",
       "      <td>-33.806252</td>\n",
       "      <td>48.382155</td>\n",
       "      <td>81.697311</td>\n",
       "      <td>-30.821435</td>\n",
       "      <td>46.589561</td>\n",
       "      <td>82.693686</td>\n",
       "      <td>-29.266909</td>\n",
       "      <td>46.941151</td>\n",
       "      <td>85.865447</td>\n",
       "      <td>-24.949200</td>\n",
       "      <td>50.942798</td>\n",
       "      <td>84.373487</td>\n",
       "      <td>-28.776171</td>\n",
       "      <td>57.177513</td>\n",
       "      <td>83.183279</td>\n",
       "      <td>-28.576656</td>\n",
       "      <td>53.669544</td>\n",
       "      <td>77.761579</td>\n",
       "      <td>-29.641994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.445896</td>\n",
       "      <td>3.823397</td>\n",
       "      <td>32.676394</td>\n",
       "      <td>40.715829</td>\n",
       "      <td>33.898217</td>\n",
       "      <td>32.353705</td>\n",
       "      <td>40.641751</td>\n",
       "      <td>34.163914</td>\n",
       "      <td>33.570688</td>\n",
       "      <td>41.417587</td>\n",
       "      <td>34.226874</td>\n",
       "      <td>34.402447</td>\n",
       "      <td>42.019487</td>\n",
       "      <td>34.992982</td>\n",
       "      <td>34.565113</td>\n",
       "      <td>43.169360</td>\n",
       "      <td>34.775522</td>\n",
       "      <td>35.985091</td>\n",
       "      <td>43.419765</td>\n",
       "      <td>35.558160</td>\n",
       "      <td>36.604571</td>\n",
       "      <td>44.187079</td>\n",
       "      <td>34.613903</td>\n",
       "      <td>39.398134</td>\n",
       "      <td>42.863033</td>\n",
       "      <td>34.522784</td>\n",
       "      <td>39.103964</td>\n",
       "      <td>44.266600</td>\n",
       "      <td>35.921888</td>\n",
       "      <td>41.614585</td>\n",
       "      <td>43.932179</td>\n",
       "      <td>37.058562</td>\n",
       "      <td>40.719084</td>\n",
       "      <td>46.033896</td>\n",
       "      <td>42.029159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-80.928512</td>\n",
       "      <td>-98.233756</td>\n",
       "      <td>-120.096446</td>\n",
       "      <td>-111.685241</td>\n",
       "      <td>-96.142589</td>\n",
       "      <td>-114.271334</td>\n",
       "      <td>-106.886524</td>\n",
       "      <td>-89.972754</td>\n",
       "      <td>-118.950653</td>\n",
       "      <td>-89.009910</td>\n",
       "      <td>-97.565346</td>\n",
       "      <td>-133.877193</td>\n",
       "      <td>-87.860871</td>\n",
       "      <td>-86.081022</td>\n",
       "      <td>-116.422479</td>\n",
       "      <td>-88.702402</td>\n",
       "      <td>-96.892390</td>\n",
       "      <td>-134.558324</td>\n",
       "      <td>-80.114463</td>\n",
       "      <td>-21.617589</td>\n",
       "      <td>-151.592200</td>\n",
       "      <td>-108.605639</td>\n",
       "      <td>-50.233962</td>\n",
       "      <td>-108.440190</td>\n",
       "      <td>-121.182089</td>\n",
       "      <td>-0.001324</td>\n",
       "      <td>-114.500502</td>\n",
       "      <td>-83.649652</td>\n",
       "      <td>-39.539982</td>\n",
       "      <td>-112.908778</td>\n",
       "      <td>-80.196289</td>\n",
       "      <td>-2.877761</td>\n",
       "      <td>-103.718071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>31.260337</td>\n",
       "      <td>60.571185</td>\n",
       "      <td>-58.246757</td>\n",
       "      <td>30.917769</td>\n",
       "      <td>62.849522</td>\n",
       "      <td>-59.477546</td>\n",
       "      <td>29.413095</td>\n",
       "      <td>56.823500</td>\n",
       "      <td>-60.427889</td>\n",
       "      <td>26.154476</td>\n",
       "      <td>52.914401</td>\n",
       "      <td>-61.011937</td>\n",
       "      <td>23.932549</td>\n",
       "      <td>46.851103</td>\n",
       "      <td>-61.949685</td>\n",
       "      <td>17.024806</td>\n",
       "      <td>47.819909</td>\n",
       "      <td>-59.795263</td>\n",
       "      <td>15.453349</td>\n",
       "      <td>48.034701</td>\n",
       "      <td>-58.605830</td>\n",
       "      <td>13.998841</td>\n",
       "      <td>54.378580</td>\n",
       "      <td>-51.551124</td>\n",
       "      <td>20.915813</td>\n",
       "      <td>45.916796</td>\n",
       "      <td>-59.907133</td>\n",
       "      <td>27.634371</td>\n",
       "      <td>44.872205</td>\n",
       "      <td>-58.906215</td>\n",
       "      <td>14.298679</td>\n",
       "      <td>37.354595</td>\n",
       "      <td>-70.016161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>55.336918</td>\n",
       "      <td>87.314804</td>\n",
       "      <td>-31.697639</td>\n",
       "      <td>55.415571</td>\n",
       "      <td>88.220022</td>\n",
       "      <td>-31.292537</td>\n",
       "      <td>56.038056</td>\n",
       "      <td>87.300265</td>\n",
       "      <td>-34.074548</td>\n",
       "      <td>55.710808</td>\n",
       "      <td>86.254136</td>\n",
       "      <td>-35.102632</td>\n",
       "      <td>55.348290</td>\n",
       "      <td>86.282664</td>\n",
       "      <td>-36.420433</td>\n",
       "      <td>52.719973</td>\n",
       "      <td>88.685799</td>\n",
       "      <td>-31.714577</td>\n",
       "      <td>51.569093</td>\n",
       "      <td>90.273587</td>\n",
       "      <td>-29.604648</td>\n",
       "      <td>51.989549</td>\n",
       "      <td>91.615694</td>\n",
       "      <td>-25.351387</td>\n",
       "      <td>59.602793</td>\n",
       "      <td>89.816121</td>\n",
       "      <td>-27.741830</td>\n",
       "      <td>63.516259</td>\n",
       "      <td>87.992227</td>\n",
       "      <td>-27.794790</td>\n",
       "      <td>62.781569</td>\n",
       "      <td>84.955399</td>\n",
       "      <td>-31.392546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>75.288905</td>\n",
       "      <td>106.228314</td>\n",
       "      <td>-5.744637</td>\n",
       "      <td>75.038670</td>\n",
       "      <td>109.185672</td>\n",
       "      <td>-4.650574</td>\n",
       "      <td>75.422747</td>\n",
       "      <td>105.403770</td>\n",
       "      <td>-6.319491</td>\n",
       "      <td>75.318645</td>\n",
       "      <td>104.339838</td>\n",
       "      <td>-5.203043</td>\n",
       "      <td>75.744093</td>\n",
       "      <td>105.154207</td>\n",
       "      <td>-7.220871</td>\n",
       "      <td>75.260199</td>\n",
       "      <td>106.390750</td>\n",
       "      <td>-1.864195</td>\n",
       "      <td>75.712522</td>\n",
       "      <td>110.399266</td>\n",
       "      <td>-0.965680</td>\n",
       "      <td>78.660546</td>\n",
       "      <td>121.178163</td>\n",
       "      <td>1.477405</td>\n",
       "      <td>80.909939</td>\n",
       "      <td>126.338919</td>\n",
       "      <td>0.410206</td>\n",
       "      <td>86.090881</td>\n",
       "      <td>126.478059</td>\n",
       "      <td>1.340623</td>\n",
       "      <td>84.088250</td>\n",
       "      <td>121.913529</td>\n",
       "      <td>6.843351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>151.586035</td>\n",
       "      <td>168.717458</td>\n",
       "      <td>113.345119</td>\n",
       "      <td>151.271413</td>\n",
       "      <td>170.209350</td>\n",
       "      <td>104.618115</td>\n",
       "      <td>149.208278</td>\n",
       "      <td>167.973416</td>\n",
       "      <td>104.590879</td>\n",
       "      <td>151.033472</td>\n",
       "      <td>168.292018</td>\n",
       "      <td>114.624261</td>\n",
       "      <td>150.507099</td>\n",
       "      <td>167.095078</td>\n",
       "      <td>112.110711</td>\n",
       "      <td>146.031461</td>\n",
       "      <td>167.127478</td>\n",
       "      <td>106.528407</td>\n",
       "      <td>151.739265</td>\n",
       "      <td>167.275662</td>\n",
       "      <td>107.176600</td>\n",
       "      <td>148.500495</td>\n",
       "      <td>167.487393</td>\n",
       "      <td>110.053853</td>\n",
       "      <td>173.906643</td>\n",
       "      <td>167.035153</td>\n",
       "      <td>119.213101</td>\n",
       "      <td>174.054403</td>\n",
       "      <td>167.196644</td>\n",
       "      <td>122.569627</td>\n",
       "      <td>149.486224</td>\n",
       "      <td>168.352478</td>\n",
       "      <td>108.211488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Class          User            X0            Y0            Z0  \\\n",
       "count  13500.000000  13500.000000  13500.000000  13500.000000  13500.000000   \n",
       "mean       3.025481      5.777778     52.698144     85.770524    -31.826014   \n",
       "std        1.445896      3.823397     32.676394     40.715829     33.898217   \n",
       "min        1.000000      0.000000    -80.928512    -98.233756   -120.096446   \n",
       "25%        2.000000      2.000000     31.260337     60.571185    -58.246757   \n",
       "50%        3.000000      6.000000     55.336918     87.314804    -31.697639   \n",
       "75%        4.000000      9.000000     75.288905    106.228314     -5.744637   \n",
       "max        5.000000     11.000000    151.586035    168.717458    113.345119   \n",
       "\n",
       "                 X1            Y1            Z1            X2            Y2  \\\n",
       "count  13500.000000  13500.000000  13500.000000  13500.000000  13500.000000   \n",
       "mean      52.351641     86.272265    -31.711906     52.112129     83.898329   \n",
       "std       32.353705     40.641751     34.163914     33.570688     41.417587   \n",
       "min     -111.685241    -96.142589   -114.271334   -106.886524    -89.972754   \n",
       "25%       30.917769     62.849522    -59.477546     29.413095     56.823500   \n",
       "50%       55.415571     88.220022    -31.292537     56.038056     87.300265   \n",
       "75%       75.038670    109.185672     -4.650574     75.422747    105.403770   \n",
       "max      151.271413    170.209350    104.618115    149.208278    167.973416   \n",
       "\n",
       "                 Z2            X3            Y3            Z3            X4  \\\n",
       "count  13500.000000  13464.000000  13464.000000  13464.000000  13254.000000   \n",
       "mean     -32.956184     51.149287     82.018282    -33.071126     50.755154   \n",
       "std       34.226874     34.402447     42.019487     34.992982     34.565113   \n",
       "min     -118.950653    -89.009910    -97.565346   -133.877193    -87.860871   \n",
       "25%      -60.427889     26.154476     52.914401    -61.011937     23.932549   \n",
       "50%      -34.074548     55.710808     86.254136    -35.102632     55.348290   \n",
       "75%       -6.319491     75.318645    104.339838     -5.203043     75.744093   \n",
       "max      104.590879    151.033472    168.292018    114.624261    150.507099   \n",
       "\n",
       "                 Y4            Z4            X5            Y5            Z5  \\\n",
       "count  13254.000000  13254.000000  11363.000000  11363.000000  11363.000000   \n",
       "mean      80.962652    -33.806252     48.382155     81.697311    -30.821435   \n",
       "std       43.169360     34.775522     35.985091     43.419765     35.558160   \n",
       "min      -86.081022   -116.422479    -88.702402    -96.892390   -134.558324   \n",
       "25%       46.851103    -61.949685     17.024806     47.819909    -59.795263   \n",
       "50%       86.282664    -36.420433     52.719973     88.685799    -31.714577   \n",
       "75%      105.154207     -7.220871     75.260199    106.390750     -1.864195   \n",
       "max      167.095078    112.110711    146.031461    167.127478    106.528407   \n",
       "\n",
       "                X6           Y6           Z6           X7           Y7  \\\n",
       "count  9616.000000  9616.000000  9616.000000  7191.000000  7191.000000   \n",
       "mean     46.589561    82.693686   -29.266909    46.941151    85.865447   \n",
       "std      36.604571    44.187079    34.613903    39.398134    42.863033   \n",
       "min     -80.114463   -21.617589  -151.592200  -108.605639   -50.233962   \n",
       "25%      15.453349    48.034701   -58.605830    13.998841    54.378580   \n",
       "50%      51.569093    90.273587   -29.604648    51.989549    91.615694   \n",
       "75%      75.712522   110.399266    -0.965680    78.660546   121.178163   \n",
       "max     151.739265   167.275662   107.176600   148.500495   167.487393   \n",
       "\n",
       "                Z7           X8           Y8           Z8           X9  \\\n",
       "count  7191.000000  5733.000000  5733.000000  5733.000000  4537.000000   \n",
       "mean    -24.949200    50.942798    84.373487   -28.776171    57.177513   \n",
       "std      34.522784    39.103964    44.266600    35.921888    41.614585   \n",
       "min    -108.440190  -121.182089    -0.001324  -114.500502   -83.649652   \n",
       "25%     -51.551124    20.915813    45.916796   -59.907133    27.634371   \n",
       "50%     -25.351387    59.602793    89.816121   -27.741830    63.516259   \n",
       "75%       1.477405    80.909939   126.338919     0.410206    86.090881   \n",
       "max     110.053853   173.906643   167.035153   119.213101   174.054403   \n",
       "\n",
       "                Y9           Z9          X10          Y10          Z10  X11  \\\n",
       "count  4537.000000  4537.000000  2482.000000  2482.000000  2482.000000  0.0   \n",
       "mean     83.183279   -28.576656    53.669544    77.761579   -29.641994  NaN   \n",
       "std      43.932179    37.058562    40.719084    46.033896    42.029159  NaN   \n",
       "min     -39.539982  -112.908778   -80.196289    -2.877761  -103.718071  NaN   \n",
       "25%      44.872205   -58.906215    14.298679    37.354595   -70.016161  NaN   \n",
       "50%      87.992227   -27.794790    62.781569    84.955399   -31.392546  NaN   \n",
       "75%     126.478059     1.340623    84.088250   121.913529     6.843351  NaN   \n",
       "max     167.196644   122.569627   149.486224   168.352478   108.211488  NaN   \n",
       "\n",
       "       Y11  Z11  \n",
       "count  0.0  0.0  \n",
       "mean   NaN  NaN  \n",
       "std    NaN  NaN  \n",
       "min    NaN  NaN  \n",
       "25%    NaN  NaN  \n",
       "50%    NaN  NaN  \n",
       "75%    NaN  NaN  \n",
       "max    NaN  NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore dataset: Ranges of data are comparable, so standardization is not required, will check again after feature extraction\n",
    "pd.set_option('display.max_columns', 500)\n",
    "train_set.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction (Training): Non-NA count, mean for each axis, STD for each axis, min for each axis, max for each axis\n",
    "#Count\n",
    "train_set['count'] = train_set[all_columns].count(axis=1)\n",
    "\n",
    "#Means\n",
    "train_set['y_mean'] = train_set[y_columns].mean(axis=1)\n",
    "train_set['z_mean'] = train_set[z_columns].mean(axis=1)\n",
    "\n",
    "#Standard Deviations\n",
    "train_set['x_std'] = train_set[x_columns].std(axis=1)\n",
    "train_set['y_std'] = train_set[y_columns].std(axis=1)\n",
    "train_set['z_std'] = train_set[z_columns].std(axis=1)\n",
    "\n",
    "#Maximum\n",
    "train_set['x_max'] = train_set[x_columns].max(axis=1)\n",
    "train_set['y_max'] = train_set[y_columns].max(axis=1)\n",
    "train_set['z_max'] = train_set[z_columns].max(axis=1)\n",
    "\n",
    "#Minimum\n",
    "train_set['x_min'] = train_set[x_columns].min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the original columns\n",
    "train_set.drop(columns=all_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>User</th>\n",
       "      <th>count</th>\n",
       "      <th>y_mean</th>\n",
       "      <th>z_mean</th>\n",
       "      <th>x_std</th>\n",
       "      <th>y_std</th>\n",
       "      <th>z_std</th>\n",
       "      <th>x_max</th>\n",
       "      <th>y_max</th>\n",
       "      <th>z_max</th>\n",
       "      <th>x_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>13500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.025481</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>24.031111</td>\n",
       "      <td>80.373621</td>\n",
       "      <td>-34.503680</td>\n",
       "      <td>32.439537</td>\n",
       "      <td>39.682642</td>\n",
       "      <td>26.076963</td>\n",
       "      <td>93.476938</td>\n",
       "      <td>130.757534</td>\n",
       "      <td>-2.775697</td>\n",
       "      <td>1.818376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.445896</td>\n",
       "      <td>3.823397</td>\n",
       "      <td>6.452347</td>\n",
       "      <td>17.246827</td>\n",
       "      <td>20.997457</td>\n",
       "      <td>8.230813</td>\n",
       "      <td>11.935065</td>\n",
       "      <td>12.011993</td>\n",
       "      <td>20.850893</td>\n",
       "      <td>26.737334</td>\n",
       "      <td>30.435922</td>\n",
       "      <td>22.985188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>-50.301817</td>\n",
       "      <td>-92.767794</td>\n",
       "      <td>2.099186</td>\n",
       "      <td>2.939049</td>\n",
       "      <td>2.246756</td>\n",
       "      <td>-27.857612</td>\n",
       "      <td>-39.663200</td>\n",
       "      <td>-72.571532</td>\n",
       "      <td>-121.182089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>67.838747</td>\n",
       "      <td>-49.998885</td>\n",
       "      <td>27.351446</td>\n",
       "      <td>32.693702</td>\n",
       "      <td>17.526247</td>\n",
       "      <td>78.941544</td>\n",
       "      <td>127.423988</td>\n",
       "      <td>-25.165756</td>\n",
       "      <td>-13.215606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>81.495304</td>\n",
       "      <td>-36.156457</td>\n",
       "      <td>33.480391</td>\n",
       "      <td>41.432292</td>\n",
       "      <td>25.250417</td>\n",
       "      <td>92.440150</td>\n",
       "      <td>139.227723</td>\n",
       "      <td>-2.521009</td>\n",
       "      <td>-3.355784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>94.251337</td>\n",
       "      <td>-18.156529</td>\n",
       "      <td>36.571780</td>\n",
       "      <td>46.865052</td>\n",
       "      <td>34.350737</td>\n",
       "      <td>108.646575</td>\n",
       "      <td>149.616753</td>\n",
       "      <td>16.989548</td>\n",
       "      <td>13.176117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>136.535435</td>\n",
       "      <td>70.051134</td>\n",
       "      <td>80.637048</td>\n",
       "      <td>72.982286</td>\n",
       "      <td>62.816154</td>\n",
       "      <td>174.054403</td>\n",
       "      <td>170.209350</td>\n",
       "      <td>122.569627</td>\n",
       "      <td>68.288056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Class          User         count        y_mean        z_mean  \\\n",
       "count  13500.000000  13500.000000  13500.000000  13500.000000  13500.000000   \n",
       "mean       3.025481      5.777778     24.031111     80.373621    -34.503680   \n",
       "std        1.445896      3.823397      6.452347     17.246827     20.997457   \n",
       "min        1.000000      0.000000      9.000000    -50.301817    -92.767794   \n",
       "25%        2.000000      2.000000     18.000000     67.838747    -49.998885   \n",
       "50%        3.000000      6.000000     24.000000     81.495304    -36.156457   \n",
       "75%        4.000000      9.000000     30.000000     94.251337    -18.156529   \n",
       "max        5.000000     11.000000     33.000000    136.535435     70.051134   \n",
       "\n",
       "              x_std         y_std         z_std         x_max         y_max  \\\n",
       "count  13500.000000  13500.000000  13500.000000  13500.000000  13500.000000   \n",
       "mean      32.439537     39.682642     26.076963     93.476938    130.757534   \n",
       "std        8.230813     11.935065     12.011993     20.850893     26.737334   \n",
       "min        2.099186      2.939049      2.246756    -27.857612    -39.663200   \n",
       "25%       27.351446     32.693702     17.526247     78.941544    127.423988   \n",
       "50%       33.480391     41.432292     25.250417     92.440150    139.227723   \n",
       "75%       36.571780     46.865052     34.350737    108.646575    149.616753   \n",
       "max       80.637048     72.982286     62.816154    174.054403    170.209350   \n",
       "\n",
       "              z_max         x_min  \n",
       "count  13500.000000  13500.000000  \n",
       "mean      -2.775697      1.818376  \n",
       "std       30.435922     22.985188  \n",
       "min      -72.571532   -121.182089  \n",
       "25%      -25.165756    -13.215606  \n",
       "50%       -2.521009     -3.355784  \n",
       "75%       16.989548     13.176117  \n",
       "max      122.569627     68.288056  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore the new features: The data is relatively on the same scale, so I choose not to standardize for now\n",
    "train_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature reduction: I'm using all the extracted features first then will explore reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to numpy (training features, training labels, and user ID)\n",
    "train_x = train_set[final_features].to_numpy()\n",
    "train_y = train_set['Class'].to_numpy()\n",
    "train_user = train_set['User'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.912962962962963\n"
     ]
    }
   ],
   "source": [
    "#Baseline Model: Naive Bayes\n",
    "NB = GaussianNB()\n",
    "NB.fit(train_x,train_y)\n",
    "print('Training Accuracy: ', accuracy_score(train_y, NB.predict(train_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Range of gamma and C parameter values for best parameter selection using cross-validation\n",
    "gamma = np.logspace(-7,-1,num=20)\n",
    "c = np.logspace(-5,1,num=20)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "\n",
    "#Arrays to store cross-validation accuracy results \n",
    "accuracies = np.zeros((trials*n,len(gamma),len(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split all users\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=178)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=178)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=178)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=178)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=178)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy()\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy()\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of gamma and C combinations with startification on the one-out set\n",
    "        for i in range(len(gamma)):\n",
    "            for j in range(len(c)):\n",
    "                svccv = SVC(kernel='rbf', gamma=gamma[i], C=c[j], cache_size=4000)\n",
    "                svccv.fit(train_x_r, train_y_r)\n",
    "                accuracies[k+N*9,i,j] = accuracy_score(test_y_r, svccv.predict(test_x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suitable pair:\tgamma= 0.0001438449888287663 C= 4.832930238571752\n",
      "\t\taccuracy= 0.9297378277153562\n"
     ]
    }
   ],
   "source": [
    "accuracies = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracies.argmax(), accuracies.shape) #Index of max accuracy in terms of the original gamma and c arrays\n",
    "print('Suitable pair:\\tgamma=', gamma[x[0]], 'C=',c[x[1]])\n",
    "print('\\t\\taccuracy=', accuracies[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9949629629629629\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='rbf', gamma=2.1544346900318823e-05, C=10)\n",
    "svm.fit(train_x,train_y)\n",
    "print('Training Accuracy: ', accuracy_score(train_y, svm.predict(train_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tolerance range\n",
    "reg = np.logspace(-7,-1,num=1000)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "\n",
    "#Arrays to store cross-validation accuracy results \n",
    "accuracies = np.zeros((trials*n,len(reg)))\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=178)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=178)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=178)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=178)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=178)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy()\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy()\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of gamma and C combinations with startification on the one-out set\n",
    "        for i in range(len(reg)):\n",
    "            perc = Perceptron(penalty='l2', alpha=reg[i], max_iter=10000000, shuffle=True)\n",
    "            perc.fit(train_x_r, train_y_r)\n",
    "            accuracies[k+N*9,i] = accuracy_score(test_y_r, perc.predict(test_x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suitable pair:\treg= 1.1169868184678226e-07\n",
      "\t\taccuracy= 0.72521847690387\n"
     ]
    }
   ],
   "source": [
    "accuracies = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracies.argmax(), accuracies.shape) #Index of max accuracy in terms of the original gamma and c arrays\n",
    "print('Suitable pair:\\treg=', reg[x[0]])\n",
    "print('\\t\\taccuracy=', accuracies[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.742\n"
     ]
    }
   ],
   "source": [
    "perc = Perceptron(tol=14.831025143361028, max_iter=10000000)\n",
    "perc.fit(train_x, train_y)\n",
    "print('Training Accuracy: ', accuracy_score(train_y, perc.predict(train_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shrinkage range\n",
    "shrinkage = np.logspace(-6,0,num=100)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "\n",
    "#Arrays to store cross-validation accuracy results \n",
    "accuracies = np.zeros((trials*n,len(shrinkage)))\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=178)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=178)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=178)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=178)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=178)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy()\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy()\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of gamma and C combinations with startification on the one-out set\n",
    "        for i in range(len(shrinkage)):\n",
    "            lda = LinearDiscriminantAnalysis(shrinkage=shrinkage[i], solver='lsqr')\n",
    "            lda.fit(train_x_r, train_y_r)\n",
    "            accuracies[k+N*9,i] = accuracy_score(test_y_r, lda.predict(test_x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suitable pair:\treg= 0.0003511191734215131\n",
      "\t\taccuracy= 0.9046192259675403\n"
     ]
    }
   ],
   "source": [
    "accuracies = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracies.argmax(), accuracies.shape) #Index of max accuracy in terms of the original gamma and c arrays\n",
    "print('Suitable pair:\\treg=', shrinkage[x[0]])\n",
    "print('\\t\\taccuracy=', accuracies[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9437037037037037\n"
     ]
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis(shrinkage=0.0054622772176843425, solver='lsqr')\n",
    "lda.fit(train_x, train_y)\n",
    "print('Training Accuracy: ', accuracy_score(train_y, lda.predict(train_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9328888888888889\n"
     ]
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis(solver='lsqr') #All solvers produce the same result, shrinkage worsens results (samples >> features)\n",
    "lda.fit(train_x, train_y)\n",
    "print('Training Accuracy: ', accuracy_score(train_y, lda.predict(train_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis (QDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularization range\n",
    "reg = np.logspace(-6,0,num=1000)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "\n",
    "#Arrays to store cross-validation accuracy results \n",
    "accuracies = np.zeros((trials*n,len(reg)))\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=178)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=178)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=178)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=178)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=178)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy()\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy()\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of gamma and C combinations with startification on the one-out set\n",
    "        for i in range(len(reg)):\n",
    "            qda = QuadraticDiscriminantAnalysis(reg_param=reg[i])\n",
    "            qda.fit(train_x_r, train_y_r)\n",
    "            accuracies[k+N*9,i] = accuracy_score(test_y_r, qda.predict(test_x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suitable pair:\treg= 0.312964801067075\n",
      "\t\taccuracy= 0.8938826466916355\n"
     ]
    }
   ],
   "source": [
    "accuracies = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracies.argmax(), accuracies.shape) #Index of max accuracy in terms of the original gamma and c arrays\n",
    "print('Suitable pair:\\treg=', reg[x[0]])\n",
    "print('\\t\\taccuracy=', accuracies[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.992\n"
     ]
    }
   ],
   "source": [
    "qda = QuadraticDiscriminantAnalysis(reg_param=0.17347593592339325)\n",
    "qda.fit(train_x, train_y)\n",
    "print('Training Accuracy: ', accuracy_score(train_y, qda.predict(train_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neighbors range\n",
    "neighbors = np.arange(1,400,1)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "\n",
    "#Arrays to store cross-validation accuracy results \n",
    "accuracies = np.zeros((trials*n,len(neighbors)))\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=178)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=178)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=178)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=178)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=178)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy()\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy()\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of gamma and C combinations with startification on the one-out set\n",
    "        for i in range(len(neighbors)):\n",
    "            knn = KNeighborsClassifier(n_neighbors=neighbors[i],n_jobs=-1)\n",
    "            knn.fit(train_x_r, train_y_r)\n",
    "            accuracies[k+N*9,i] = accuracy_score(test_y_r, knn.predict(test_x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suitable pair:\treg= 111\n",
      "\t\taccuracy= 0.7677902621722846\n"
     ]
    }
   ],
   "source": [
    "accuracies = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracies.argmax(), accuracies.shape) #Index of max accuracy in terms of the original gamma and c arrays\n",
    "print('Suitable pair:\\treg=', neighbors[x[0]])\n",
    "print('\\t\\taccuracy=', accuracies[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1,n_jobs=-1,weights='distance')\n",
    "knn.fit(train_x, train_y)\n",
    "print('Training Accuracy: ', accuracy_score(train_y, knn.predict(train_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Range of C parameter values for best parameter selection using cross-validation\n",
    "c = np.logspace(-5,1,num=50)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "\n",
    "#Arrays to store cross-validation accuracy results \n",
    "accuracies = np.zeros((trials*n,len(c)))\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=178)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=178)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=178)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=178)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=178)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy()\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy()\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of C values with startification on the one-out set\n",
    "        for i in range(len(c)):\n",
    "            svccv = SVC(kernel='linear', C=c[i], cache_size=1000)\n",
    "            svccv.fit(train_x_r, train_y_r)\n",
    "            accuracies[k+N*9,i] = accuracy_score(test_y_r, svccv.predict(test_x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suitable pair:\tC= 0.00022229964825261955\n",
      "\t\taccuracy= 0.8953807740324594\n"
     ]
    }
   ],
   "source": [
    "accuracies = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracies.argmax(), accuracies.shape) #Index of max accuracy in terms of the original gamma and c arrays\n",
    "print('Suitable pair:\\tC=', c[x[0]])\n",
    "print('\\t\\taccuracy=', accuracies[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9911851851851852\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel='linear', C=0.001)\n",
    "svc.fit(train_x, train_y)\n",
    "print('Training Accuracy: ', accuracy_score(train_y, svc.predict(train_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try to standardize our best model (SVM linear) - the SVM is scale variant so it might benefit\n",
    "stand = preprocessing.StandardScaler().fit(train_x)\n",
    "train_x_stand = stand.transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "accuracies = np.zeros((trials*n))\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=50)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=50)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=50)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=50)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=50)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = stand.transform(balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy())\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = stand.transform(balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy())\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of C values with startification on the one-out set\n",
    "        svccv = SVC(kernel='linear')\n",
    "        svccv.fit(train_x_r, train_y_r)\n",
    "        accuracies[k+N*9] = accuracy_score(test_y_r, svccv.predict(test_x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\taccuracy= 0.8296\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracy.argmax(), accuracy.shape) #Index of max accuracy\n",
    "print('\\t\\taccuracy=', accuracy[x]) #No improvement noted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA F-value - LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\taccuracy= 0.8934222222222222\n"
     ]
    }
   ],
   "source": [
    "red = SelectKBest(f_classif, k=9).fit(train_x, train_y)\n",
    "train_x_reduced = red.transform(train_x)\n",
    "lda = LinearDiscriminantAnalysis(solver='lsqr') #All solvers produce the same result, shrinkage worsens results (samples >> features)\n",
    "lda.fit(train_x_reduced, train_y)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "accuracies = np.zeros((trials*n))\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=50)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=50)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=50)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=50)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=50)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = red.transform(balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy())\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = red.transform(balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy())\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of C values with startification on the one-out set\n",
    "        svccv = SVC(kernel='linear')\n",
    "        svccv.fit(train_x_r, train_y_r)\n",
    "        accuracies[k+N*9] = accuracy_score(test_y_r, svccv.predict(test_x_r))\n",
    "accuracy = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracy.argmax(), accuracy.shape) #Index of max accuracy\n",
    "print('\\t\\taccuracy=', accuracy[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\taccuracy= 0.8337777777777775\n"
     ]
    }
   ],
   "source": [
    "red = SelectKBest(f_classif, k=8).fit(train_x, train_y)\n",
    "train_x_reduced = red.transform(train_x)\n",
    "lda = LinearDiscriminantAnalysis(solver='lsqr') #All solvers produce the same result, shrinkage worsens results (samples >> features)\n",
    "lda.fit(train_x_reduced, train_y)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "accuracies = np.zeros((trials*n))\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=50)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=50)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=50)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=50)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=50)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = red.transform(balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy())\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = red.transform(balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy())\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of C values with startification on the one-out set\n",
    "        svccv = SVC(kernel='linear')\n",
    "        svccv.fit(train_x_r, train_y_r)\n",
    "        accuracies[k+N*9] = accuracy_score(test_y_r, svccv.predict(test_x_r))\n",
    "accuracy = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracy.argmax(), accuracy.shape) #Index of max accuracy\n",
    "print('\\t\\taccuracy=', accuracy[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\taccuracy= 0.8232000000000003\n"
     ]
    }
   ],
   "source": [
    "red = SelectKBest(f_classif, k=7).fit(train_x, train_y)\n",
    "train_x_reduced = red.transform(train_x)\n",
    "lda = LinearDiscriminantAnalysis(solver='lsqr') #All solvers produce the same result, shrinkage worsens results (samples >> features)\n",
    "lda.fit(train_x_reduced, train_y)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "accuracies = np.zeros((trials*n))\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=50)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=50)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=50)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=50)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=50)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = red.transform(balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy())\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = red.transform(balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy())\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of C values with startification on the one-out set\n",
    "        svccv = SVC(kernel='linear')\n",
    "        svccv.fit(train_x_r, train_y_r)\n",
    "        accuracies[k+N*9] = accuracy_score(test_y_r, svccv.predict(test_x_r))\n",
    "accuracy = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracy.argmax(), accuracy.shape) #Index of max accuracy\n",
    "print('\\t\\taccuracy=', accuracy[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features are already reduced on this file, these are just sample runs on additional feature reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual information (MI) -  LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\taccuracy= 0.8570666666666666\n"
     ]
    }
   ],
   "source": [
    "red = SelectKBest(mutual_info_classif, k=9).fit(train_x, train_y)\n",
    "train_x_reduced = red.transform(train_x)\n",
    "lda = LinearDiscriminantAnalysis(solver='lsqr') #All solvers produce the same result, shrinkage worsens results (samples >> features)\n",
    "lda.fit(train_x_reduced, train_y)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "accuracies = np.zeros((trials*n))\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=50)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=50)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=50)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=50)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=50)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = red.transform(balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy())\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = red.transform(balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy())\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of C values with startification on the one-out set\n",
    "        svccv = SVC(kernel='linear')\n",
    "        svccv.fit(train_x_r, train_y_r)\n",
    "        accuracies[k+N*9] = accuracy_score(test_y_r, svccv.predict(test_x_r))\n",
    "accuracy = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracy.argmax(), accuracy.shape) #Index of max accuracy\n",
    "print('\\t\\taccuracy=', accuracy[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\taccuracy= 0.847111111111111\n"
     ]
    }
   ],
   "source": [
    "red = SelectKBest(mutual_info_classif, k=8).fit(train_x, train_y)\n",
    "train_x_reduced = red.transform(train_x)\n",
    "lda = LinearDiscriminantAnalysis(solver='lsqr') #All solvers produce the same result, shrinkage worsens results (samples >> features)\n",
    "lda.fit(train_x_reduced, train_y)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "accuracies = np.zeros((trials*n))\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=50)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=50)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=50)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=50)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=50)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = red.transform(balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy())\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = red.transform(balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy())\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of C values with startification on the one-out set\n",
    "        svccv = SVC(kernel='linear')\n",
    "        svccv.fit(train_x_r, train_y_r)\n",
    "        accuracies[k+N*9] = accuracy_score(test_y_r, svccv.predict(test_x_r))\n",
    "accuracy = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracy.argmax(), accuracy.shape) #Index of max accuracy\n",
    "print('\\t\\taccuracy=', accuracy[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\taccuracy= 0.8311111111111111\n"
     ]
    }
   ],
   "source": [
    "red = SelectKBest(mutual_info_classif, k=7).fit(train_x, train_y)\n",
    "train_x_reduced = red.transform(train_x)\n",
    "lda = LinearDiscriminantAnalysis(solver='lsqr') #All solvers produce the same result, shrinkage worsens results (samples >> features)\n",
    "lda.fit(train_x_reduced, train_y)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "accuracies = np.zeros((trials*n))\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=50)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=50)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=50)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=50)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=50)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = red.transform(balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy())\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = red.transform(balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy())\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of C values with startification on the one-out set\n",
    "        svccv = SVC(kernel='linear')\n",
    "        svccv.fit(train_x_r, train_y_r)\n",
    "        accuracies[k+N*9] = accuracy_score(test_y_r, svccv.predict(test_x_r))\n",
    "accuracy = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracy.argmax(), accuracy.shape) #Index of max accuracy\n",
    "print('\\t\\taccuracy=', accuracy[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two features removed also: zmin and xmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the two results: 10 features - LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results already combined, this is just a sample run of approach\n",
    "#train_x_reduced = train_x[:,np.array([0,2,3,4,5,6,7,8,9,10])]\n",
    "#test_x_reduced = test_x[:,np.array([0,2,3,4,5,6,7,8,9,10])]\n",
    "#lda = LinearDiscriminantAnalysis(solver='lsqr') #All solvers produce the same result, shrinkage worsens results (samples >> features)\n",
    "#lda.fit(train_x_reduced, train_y)\n",
    "#print('Training Accuracy: ', accuracy_score(train_y, lda.predict(train_x_reduced)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA F-value - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\taccuracy= 0.859111111111111\n"
     ]
    }
   ],
   "source": [
    "red = SelectKBest(f_classif, k=9).fit(train_x, train_y)\n",
    "train_x_reduced = red.transform(train_x)\n",
    "svc = LinearDiscriminantAnalysis(solver='lsqr') #All solvers produce the same result, shrinkage worsens results (samples >> features)\n",
    "svc.fit(train_x_reduced, train_y)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "accuracies = np.zeros((trials*n))\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=50)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=50)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=50)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=50)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=50)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = red.transform(balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy())\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = red.transform(balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy())\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of C values with startification on the one-out set\n",
    "        svccv = SVC(kernel='linear')\n",
    "        svccv.fit(train_x_r, train_y_r)\n",
    "        accuracies[k+N*9] = accuracy_score(test_y_r, svccv.predict(test_x_r))\n",
    "accuracy = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracy.argmax(), accuracy.shape) #Index of max accuracy\n",
    "print('\\t\\taccuracy=', accuracy[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\taccuracy= 0.8488888888888888\n"
     ]
    }
   ],
   "source": [
    "red = SelectKBest(f_classif, k=8).fit(train_x, train_y)\n",
    "train_x_reduced = red.transform(train_x)\n",
    "svc = LinearDiscriminantAnalysis(solver='lsqr') #All solvers produce the same result, shrinkage worsens results (samples >> features)\n",
    "svc.fit(train_x_reduced, train_y)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "accuracies = np.zeros((trials*n))\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=50)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=50)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=50)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=50)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=50)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = red.transform(balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy())\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = red.transform(balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy())\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of C values with startification on the one-out set\n",
    "        svccv = SVC(kernel='linear')\n",
    "        svccv.fit(train_x_r, train_y_r)\n",
    "        accuracies[k+N*9] = accuracy_score(test_y_r, svccv.predict(test_x_r))\n",
    "accuracy = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracy.argmax(), accuracy.shape) #Index of max accuracy\n",
    "print('\\t\\taccuracy=', accuracy[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\taccuracy= 0.7888888888888889\n"
     ]
    }
   ],
   "source": [
    "red = SelectKBest(f_classif, k=7).fit(train_x, train_y)\n",
    "train_x_reduced = red.transform(train_x)\n",
    "svc = LinearDiscriminantAnalysis(solver='lsqr') #All solvers produce the same result, shrinkage worsens results (samples >> features)\n",
    "svc.fit(train_x_reduced, train_y)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "accuracies = np.zeros((trials*n))\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=50)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=50)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=50)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=50)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=50)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = red.transform(balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy())\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = red.transform(balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy())\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of C values with startification on the one-out set\n",
    "        svccv = SVC(kernel='linear')\n",
    "        svccv.fit(train_x_r, train_y_r)\n",
    "        accuracies[k+N*9] = accuracy_score(test_y_r, svccv.predict(test_x_r))\n",
    "accuracy = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracy.argmax(), accuracy.shape) #Index of max accuracy\n",
    "print('\\t\\taccuracy=', accuracy[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual information (MI) -  SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\taccuracy= 0.8475555555555556\n"
     ]
    }
   ],
   "source": [
    "red = SelectKBest(mutual_info_classif, k=9).fit(train_x, train_y)\n",
    "train_x_reduced = red.transform(train_x)\n",
    "svc = LinearDiscriminantAnalysis(solver='lsqr') #All solvers produce the same result, shrinkage worsens results (samples >> features)\n",
    "svc.fit(train_x_reduced, train_y)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "accuracies = np.zeros((trials*n))\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=50)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=50)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=50)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=50)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=50)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = red.transform(balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy())\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = red.transform(balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy())\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of C values with startification on the one-out set\n",
    "        svccv = SVC(kernel='linear')\n",
    "        svccv.fit(train_x_r, train_y_r)\n",
    "        accuracies[k+N*9] = accuracy_score(test_y_r, svccv.predict(test_x_r))\n",
    "accuracy = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracy.argmax(), accuracy.shape) #Index of max accuracy\n",
    "print('\\t\\taccuracy=', accuracy[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\taccuracy= 0.8640000000000001\n"
     ]
    }
   ],
   "source": [
    "red = SelectKBest(mutual_info_classif, k=8).fit(train_x, train_y)\n",
    "train_x_reduced = red.transform(train_x)\n",
    "svc = LinearDiscriminantAnalysis(solver='lsqr') #All solvers produce the same result, shrinkage worsens results (samples >> features)\n",
    "svc.fit(train_x_reduced, train_y)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "accuracies = np.zeros((trials*n))\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=50)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=50)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=50)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=50)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=50)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = red.transform(balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy())\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = red.transform(balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy())\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of C values with startification on the one-out set\n",
    "        svccv = SVC(kernel='linear')\n",
    "        svccv.fit(train_x_r, train_y_r)\n",
    "        accuracies[k+N*9] = accuracy_score(test_y_r, svccv.predict(test_x_r))\n",
    "accuracy = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracy.argmax(), accuracy.shape) #Index of max accuracy\n",
    "print('\\t\\taccuracy=', accuracy[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\taccuracy= 0.783111111111111\n"
     ]
    }
   ],
   "source": [
    "red = SelectKBest(mutual_info_classif, k=7).fit(train_x, train_y)\n",
    "train_x_reduced = red.transform(train_x)\n",
    "svc = LinearDiscriminantAnalysis(solver='lsqr') #All solvers produce the same result, shrinkage worsens results (samples >> features)\n",
    "svc.fit(train_x_reduced, train_y)\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 5\n",
    "accuracies = np.zeros((trials*n))\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Cross-Validation\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=50)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=50)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=50)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=50)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=50)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = red.transform(balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy())\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = red.transform(balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy())\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        #Cross-validate a range of C values with startification on the one-out set\n",
    "        svccv = SVC(kernel='linear')\n",
    "        svccv.fit(train_x_r, train_y_r)\n",
    "        accuracies[k+N*9] = accuracy_score(test_y_r, svccv.predict(test_x_r))\n",
    "accuracy = np.mean(accuracies,axis=0) #Average out the different cross-validation runs\n",
    "x = np.unravel_index(accuracy.argmax(), accuracy.shape) #Index of max accuracy\n",
    "print('\\t\\taccuracy=', accuracy[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the two results: 10 features - LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results already combined, this is just a test run of method\n",
    "#train_x_reduced = train_x[:,np.array([0,2,3,4,5,6,7,8,9,10])]\n",
    "#svc = LinearDiscriminantAnalysis(solver='lsqr') #All solvers produce the same result, shrinkage worsens results (samples >> features)\n",
    "#svc.fit(train_x_reduced, train_y)\n",
    "#print('Training Accuracy: ', accuracy_score(train_y, svc.predict(train_x_reduced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can safely discard x-mean, z-min, and y-min throughout the analysis as they share information already in other features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
