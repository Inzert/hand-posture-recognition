{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training and testing datasets\n",
    "train_set = pd.read_csv('./HG_Dataset/D_train.csv')\n",
    "train_set_large = pd.read_csv('./HG_Dataset/D_train_large.csv')\n",
    "test_set = pd.read_csv('./HG_Dataset/D_test.csv')\n",
    "\n",
    "#Column names for filtering purposes\n",
    "all_columns = ['X0','Y0','Z0','X1','Y1','Z1','X2','Y2','Z2','X3','Y3','Z3',\\\n",
    "           'X4','Y4','Z4','X5','Y5','Z5','X6','Y6','Z6','X7','Y7','Z7',\\\n",
    "           'X8','Y8','Z8','X9','Y9','Z9','X10','Y10','Z10','X11','Y11','Z11']\n",
    "x_columns = ['X0','X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11']\n",
    "y_columns = ['Y0','Y1','Y2','Y3','Y4','Y5','Y6','Y7','Y8','Y9','Y10','Y11']\n",
    "z_columns = ['Z0','Z1','Z2','Z3','Z4','Z5','Z6','Z7','Z8','Z9','Z10','Z11']\n",
    "final_features = ['count','y_mean','z_mean','x_std','y_std','z_std',\\\n",
    "                 'x_max','y_max','z_max','x_min']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing: Delete the unique identifier column\n",
    "train_set.drop(columns='Unnamed: 0', inplace=True)\n",
    "test_set.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction (Training): Reduced set from the feature selection performed in previous run\n",
    "#Count\n",
    "train_set['count'] = train_set[all_columns].count(axis=1)\n",
    "\n",
    "#Means\n",
    "train_set['y_mean'] = train_set[y_columns].mean(axis=1)\n",
    "train_set['z_mean'] = train_set[z_columns].mean(axis=1)\n",
    "\n",
    "#Standard Deviations\n",
    "train_set['x_std'] = train_set[x_columns].std(axis=1)\n",
    "train_set['y_std'] = train_set[y_columns].std(axis=1)\n",
    "train_set['z_std'] = train_set[z_columns].std(axis=1)\n",
    "\n",
    "#Maximum\n",
    "train_set['x_max'] = train_set[x_columns].max(axis=1)\n",
    "train_set['y_max'] = train_set[y_columns].max(axis=1)\n",
    "train_set['z_max'] = train_set[z_columns].max(axis=1)\n",
    "\n",
    "#Minimum\n",
    "train_set['x_min'] = train_set[x_columns].min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction (Training - Large): Reduced set from the feature selection performed in previous run\n",
    "#Count\n",
    "train_set_large['count'] = train_set_large[all_columns].count(axis=1)\n",
    "\n",
    "#Means\n",
    "train_set_large['y_mean'] = train_set_large[y_columns].mean(axis=1)\n",
    "train_set_large['z_mean'] = train_set_large[z_columns].mean(axis=1)\n",
    "\n",
    "#Standard Deviations\n",
    "train_set_large['x_std'] = train_set_large[x_columns].std(axis=1)\n",
    "train_set_large['y_std'] = train_set_large[y_columns].std(axis=1)\n",
    "train_set_large['z_std'] = train_set_large[z_columns].std(axis=1)\n",
    "\n",
    "#Maximum\n",
    "train_set_large['x_max'] = train_set_large[x_columns].max(axis=1)\n",
    "train_set_large['y_max'] = train_set_large[y_columns].max(axis=1)\n",
    "train_set_large['z_max'] = train_set_large[z_columns].max(axis=1)\n",
    "\n",
    "#Minimum\n",
    "train_set_large['x_min'] = train_set_large[x_columns].min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction (Testing): Reduced set from the feature selection performed in previous run\n",
    "#Count\n",
    "test_set['count'] = test_set[all_columns].count(axis=1)\n",
    "\n",
    "#Means\n",
    "test_set['y_mean'] = test_set[y_columns].mean(axis=1)\n",
    "test_set['z_mean'] = test_set[z_columns].mean(axis=1)\n",
    "\n",
    "#Standard Deviations\n",
    "test_set['x_std'] = test_set[x_columns].std(axis=1)\n",
    "test_set['y_std'] = test_set[y_columns].std(axis=1)\n",
    "test_set['z_std'] = test_set[z_columns].std(axis=1)\n",
    "\n",
    "#Maximum\n",
    "test_set['x_max'] = test_set[x_columns].max(axis=1)\n",
    "test_set['y_max'] = test_set[y_columns].max(axis=1)\n",
    "test_set['z_max'] = test_set[z_columns].max(axis=1)\n",
    "\n",
    "#Minimum\n",
    "test_set['x_min'] = test_set[x_columns].min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the original columns\n",
    "train_set.drop(columns=all_columns, inplace=True)\n",
    "train_set_large.drop(columns=all_columns, inplace=True)\n",
    "test_set.drop(columns=all_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to numpy (training features, training labels, and user ID)\n",
    "train_x = train_set[final_features].to_numpy()\n",
    "train_y = train_set['Class'].to_numpy()\n",
    "train_user = train_set['User'].to_numpy()\n",
    "train_x_large = train_set_large[final_features].to_numpy()\n",
    "train_y_large = train_set_large['Class'].to_numpy()\n",
    "train_user_large = train_set_large['User'].to_numpy()\n",
    "test_x = test_set[final_features].to_numpy()\n",
    "test_y = test_set['Class'].to_numpy()\n",
    "test_user = test_set['User'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Validation\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 10\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Arrays to store cross-validation accuracy results \n",
    "cv_acc = np.zeros((trials*n))\n",
    "\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=178)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=178)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=178)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=178)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=178)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy()\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy()\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        NB = GaussianNB()\n",
    "        NB.fit(train_x_r, train_y_r)\n",
    "        cv_acc[k+N*9] = accuracy_score(test_y_r, NB.predict(test_x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: Mean: 0.787390761548065  STD:  0.15850156979235325\n",
      "Training Accuracy:  0.912962962962963\n",
      "Testing Accuracy:  0.8038295653822456\n"
     ]
    }
   ],
   "source": [
    "#Baseline Model: Naive Bayes\n",
    "NB = GaussianNB()\n",
    "NB.fit(train_x,train_y)\n",
    "print('Cross-Validation Accuracy: Mean:', np.mean(cv_acc,axis=0), ' STD: ',np.std(cv_acc,axis=0))\n",
    "print('Training Accuracy: ', accuracy_score(train_y, NB.predict(train_x)))\n",
    "print('Testing Accuracy: ', accuracy_score(test_y, NB.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Test):\n",
      "[[4311   48  107    0    0]\n",
      " [  20 3675   19   28  660]\n",
      " [ 107    0 2622  145 1905]\n",
      " [   0  129    1 2935  849]\n",
      " [   0   27   77   17 3417]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix (Test):')\n",
    "print(confusion_matrix(test_y, NB.predict(test_x)))\n",
    "np.savetxt('NB.csv', confusion_matrix(test_y, NB.predict(test_x)), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Validation\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 10\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Arrays to store cross-validation accuracy results \n",
    "cv_acc = np.zeros((trials*n))\n",
    "\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=178)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=178)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=178)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=178)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=178)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy()\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy()\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        svm = SVC(kernel='rbf', gamma=0.0001438449888287663, C=4.832930238571752)\n",
    "        svm.fit(train_x_r, train_y_r)\n",
    "        cv_acc[k+N*9] = accuracy_score(test_y_r, svm.predict(test_x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: Mean: 0.9196754057428216  STD:  0.08613291667338556\n",
      "Training Accuracy:  0.9991111111111111\n",
      "Testing Accuracy:  0.8044931039385753\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='rbf', gamma=0.0001438449888287663, C=4.832930238571752)\n",
    "svm.fit(train_x,train_y)\n",
    "print('Cross-Validation Accuracy: Mean:', np.mean(cv_acc,axis=0), ' STD: ',np.std(cv_acc,axis=0))\n",
    "print('Training Accuracy: ', accuracy_score(train_y, svm.predict(train_x)))\n",
    "print('Testing Accuracy: ', accuracy_score(test_y, svm.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Test):\n",
      "[[4350   48    3    7   58]\n",
      " [  36 2979  538    0  849]\n",
      " [   0    0 4650   85   44]\n",
      " [   0    0  996 1801 1117]\n",
      " [   0   63    2  279 3194]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix (Test):')\n",
    "print(confusion_matrix(test_y, svm.predict(test_x)))\n",
    "np.savetxt('svm.csv', confusion_matrix(test_y, svm.predict(test_x)), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Validation\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 10\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Arrays to store cross-validation accuracy results \n",
    "cv_acc = np.zeros((trials*n))\n",
    "\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=178)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=178)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=178)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=178)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=178)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy()\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy()\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        perc = Perceptron(shuffle=True)\n",
    "        perc.fit(train_x_r, train_y_r)\n",
    "        cv_acc[k+N*9] = accuracy_score(test_y_r, perc.predict(test_x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: Mean: 0.6847440699126092  STD:  0.12705674170182155\n",
      "Training Accuracy:  0.7500740740740741\n",
      "Testing Accuracy:  0.7514574150433669\n"
     ]
    }
   ],
   "source": [
    "perc = Perceptron(shuffle=True)\n",
    "perc.fit(train_x, train_y)\n",
    "print('Cross-Validation Accuracy: Mean:', np.mean(cv_acc,axis=0), ' STD: ',np.std(cv_acc,axis=0))\n",
    "print('Training Accuracy: ', accuracy_score(train_y, perc.predict(train_x)))\n",
    "print('Testing Accuracy: ', accuracy_score(test_y, perc.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Test):\n",
      "[[4231   48  163    0   24]\n",
      " [   2 4359   10    0   31]\n",
      " [  51  514 4136   21   57]\n",
      " [   1 2701  223  989    0]\n",
      " [ 163 1221   14    0 2140]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix (Test):')\n",
    "print(confusion_matrix(test_y, perc.predict(test_x)))\n",
    "np.savetxt('perc.csv', confusion_matrix(test_y, perc.predict(test_x)), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9051851851851852\n",
      "Testing Accuracy:  0.8138774349495237\n"
     ]
    }
   ],
   "source": [
    "#Testing the onevsone approach\n",
    "ovo = OneVsOneClassifier(Perceptron())\n",
    "ovo.fit(train_x, train_y)\n",
    "print('Training Accuracy: ', accuracy_score(train_y, ovo.predict(train_x)))\n",
    "print('Testing Accuracy: ', accuracy_score(test_y, ovo.predict(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Validation\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 10\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Arrays to store cross-validation accuracy results \n",
    "cv_acc = np.zeros((trials*n))\n",
    "\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=178)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=178)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=178)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=178)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=178)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy()\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy()\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "    \n",
    "        lda = LinearDiscriminantAnalysis(solver='lsqr')\n",
    "        lda.fit(train_x_r, train_y_r)\n",
    "        cv_acc[k+N*9] = accuracy_score(test_y_r, lda.predict(test_x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: Mean: 0.9084893882646693  STD:  0.06565770843907735\n",
      "Training Accuracy:  0.9328888888888889\n",
      "Testing Accuracy:  0.9242618133560833\n"
     ]
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis(solver='lsqr') #All solvers produce the same result, shrinkage worsens results (samples >> features)\n",
    "lda.fit(train_x, train_y)\n",
    "print('Cross-Validation Accuracy: Mean:', np.mean(cv_acc,axis=0), ' STD: ',np.std(cv_acc,axis=0))\n",
    "print('Training Accuracy: ', accuracy_score(train_y, lda.predict(train_x)))\n",
    "print('Testing Accuracy: ', accuracy_score(test_y, lda.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Test):\n",
      "[[4355   48   63    0    0]\n",
      " [  33 4247   35   85    2]\n",
      " [  48    0 3657 1074    0]\n",
      " [   0   59    0 3855    0]\n",
      " [   5   17    0  129 3387]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix (Test):')\n",
    "print(confusion_matrix(test_y, lda.predict(test_x)))\n",
    "np.savetxt('lda.csv', confusion_matrix(test_y, lda.predict(test_x)), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis (QDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Validation\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 10\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Arrays to store cross-validation accuracy results \n",
    "cv_acc = np.zeros((trials*n))\n",
    "\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=178)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=178)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=178)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=178)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=178)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy()\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy()\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "\n",
    "        qda = QuadraticDiscriminantAnalysis(reg_param=0.5008407989848213)\n",
    "        qda.fit(train_x_r, train_y_r)\n",
    "        cv_acc[k+N*9] = accuracy_score(test_y_r, qda.predict(test_x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: Mean: 0.8993757802746567  STD:  0.0900826709277895\n",
      "Training Accuracy:  0.9897037037037038\n",
      "Testing Accuracy:  0.9147352955116356\n"
     ]
    }
   ],
   "source": [
    "qda = QuadraticDiscriminantAnalysis(reg_param=0.5008407989848213)\n",
    "qda.fit(train_x, train_y)\n",
    "print('Cross-Validation Accuracy: Mean:', np.mean(cv_acc,axis=0), ' STD: ',np.std(cv_acc,axis=0))\n",
    "print('Training Accuracy: ', accuracy_score(train_y, qda.predict(train_x)))\n",
    "print('Testing Accuracy: ', accuracy_score(test_y, qda.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Test):\n",
      "[[4212   48   98   74   34]\n",
      " [   0 4272  128    2    0]\n",
      " [ 152    0 4491  136    0]\n",
      " [   0  554  374 2951   35]\n",
      " [   7   29   46   82 3374]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix (Test):')\n",
    "print(confusion_matrix(test_y, qda.predict(test_x)))\n",
    "np.savetxt('qda.csv', confusion_matrix(test_y, qda.predict(test_x)), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Validation\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 10\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Arrays to store cross-validation accuracy results \n",
    "cv_acc = np.zeros((trials*n))\n",
    "\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=178)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=178)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=178)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=178)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=178)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy()\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy()\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=1,n_jobs=-1)\n",
    "        knn.fit(train_x_r, train_y_r)\n",
    "        cv_acc[k+N*9] = accuracy_score(test_y_r, knn.predict(test_x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: Mean: 0.7548064918851435  STD:  0.198558947188591\n",
      "Training Accuracy:  1.0\n",
      "Testing Accuracy:  0.6406464761363098\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1,n_jobs=-1)\n",
    "knn.fit(train_x, train_y)\n",
    "print('Cross-Validation Accuracy: Mean:', np.mean(cv_acc,axis=0), ' STD: ',np.std(cv_acc,axis=0))\n",
    "print('Training Accuracy: ', accuracy_score(train_y, knn.predict(train_x)))\n",
    "print('Testing Accuracy: ', accuracy_score(test_y, knn.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Test):\n",
      "[[3273   25    1    3 1164]\n",
      " [   5 3462   66    0  869]\n",
      " [   1  740 1864  259 1915]\n",
      " [   0   20  922 1586 1386]\n",
      " [   0   42   20  144 3332]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix (Test):')\n",
    "print(confusion_matrix(test_y, knn.predict(test_x)))\n",
    "np.savetxt('knn.csv', confusion_matrix(test_y, knn.predict(test_x)), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Validation\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 10\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Arrays to store cross-validation accuracy results \n",
    "cv_acc = np.zeros((trials*n))\n",
    "\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=178)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=178)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=178)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=178)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=178)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy()\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy()\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "\n",
    "        svc = SVC(kernel='linear', C=0.00029470517025518097)\n",
    "        svc.fit(train_x_r, train_y_r)\n",
    "        cv_acc[k+N*9] = accuracy_score(test_y_r, svc.predict(test_x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: Mean: 0.8818726591760299  STD:  0.1023991133762062\n",
      "Training Accuracy:  0.9864444444444445\n",
      "Testing Accuracy:  0.9224607801317598\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel='linear', C=0.00029470517025518097)\n",
    "svc.fit(train_x, train_y)\n",
    "print('Cross-Validation Accuracy: Mean:', np.mean(cv_acc,axis=0), ' STD: ',np.std(cv_acc,axis=0))\n",
    "print('Training Accuracy: ', accuracy_score(train_y, svc.predict(train_x)))\n",
    "print('Testing Accuracy: ', accuracy_score(test_y, svc.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Test):\n",
      "[[4341   48   64    0   13]\n",
      " [  36 4318   31   15    2]\n",
      " [  19   45 4427  288    0]\n",
      " [   0  907   63 2936    8]\n",
      " [   0   61    0   36 3441]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix (Test):')\n",
    "print(confusion_matrix(test_y, svc.predict(test_x)))\n",
    "np.savetxt('svc.csv', confusion_matrix(test_y, svc.predict(test_x)), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier - Four Classifiers Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Validation\n",
    "\n",
    "#List of available training users to drop one by one in cross validation\n",
    "users = [0,1,2,5,6,8,9,10,11]\n",
    "n = len(users)\n",
    "trials = 10\n",
    "\n",
    "user_df = []\n",
    "for i in range(n):\n",
    "    user_df.append(train_set.loc[train_set['User'] == users[i]])\n",
    "\n",
    "#Arrays to store cross-validation accuracy results \n",
    "cv_acc = np.zeros((trials*n))\n",
    "\n",
    "for N in range(trials):\n",
    "    for k in range(n):\n",
    "        #Create a shuffled, balanced set from the data\n",
    "        for j in range(n):\n",
    "            class1 = user_df[j].loc[user_df[j]['Class'] == 1].sample(n=178)\n",
    "            class2 = user_df[j].loc[user_df[j]['Class'] == 2].sample(n=178)\n",
    "            class3 = user_df[j].loc[user_df[j]['Class'] == 3].sample(n=178)\n",
    "            class4 = user_df[j].loc[user_df[j]['Class'] == 4].sample(n=178)\n",
    "            class5 = user_df[j].loc[user_df[j]['Class'] == 5].sample(n=178)\n",
    "            user_df[j] = pd.concat([class1, class2, class3, class4, class5])\n",
    "    \n",
    "        balanced_set = pd.concat([user_df[0],user_df[1],user_df[2],user_df[3],user_df[4],\\\n",
    "                                  user_df[5],user_df[6],user_df[7],user_df[8]])\n",
    "\n",
    "        #Remove one user from the training set and use it for validation, train using the balanced remainder\n",
    "        train_x_r = balanced_set.loc[balanced_set['User'] != users[k]][final_features].to_numpy()\n",
    "        train_y_r = balanced_set.loc[balanced_set['User'] != users[k]]['Class'].to_numpy()\n",
    "        test_x_r = balanced_set.loc[balanced_set['User'] == users[k]][final_features].to_numpy()\n",
    "        test_y_r = balanced_set.loc[balanced_set['User'] == users[k]]['Class'].to_numpy()\n",
    "\n",
    "        ensemble = VotingClassifier(estimators=[('svm', svm), ('lda', lda), ('qda', qda)],voting='hard')\n",
    "        ensemble.fit(train_x_r, train_y_r)\n",
    "        cv_acc[k+N*9] = accuracy_score(test_y_r, ensemble.predict(test_x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: Mean: 0.9158551810237204  STD:  0.07909176762788242\n",
      "Training Accuracy:  0.9951851851851852\n",
      "Testing Accuracy:  0.9539788615574197\n"
     ]
    }
   ],
   "source": [
    "ensemble = VotingClassifier(estimators=[('svm', svm), ('lda', lda), ('qda', qda)],voting='hard')\n",
    "ensemble.fit(train_x, train_y)\n",
    "print('Cross-Validation Accuracy: Mean:', np.mean(cv_acc,axis=0), ' STD: ',np.std(cv_acc,axis=0))\n",
    "print('Training Accuracy: ', accuracy_score(train_y, ensemble.predict(train_x)))\n",
    "print('Testing Accuracy: ', accuracy_score(test_y, ensemble.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Test):\n",
      "[[4350   48   49    0   19]\n",
      " [  33 4282   84    1    2]\n",
      " [  49    0 4651   79    0]\n",
      " [   0  163  304 3423   24]\n",
      " [   7   59    9   41 3422]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix (Test):')\n",
    "print(confusion_matrix(test_y, ensemble.predict(test_x)))\n",
    "np.savetxt('ensemble.csv', confusion_matrix(test_y, ensemble.predict(test_x)), delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
